{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b471ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: 3950\n",
      "   idx      class                                       conversation\n",
      "0    0      협박 대화  지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...\n",
      "1    1      협박 대화  길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...\n",
      "2    2  기타 괴롭힘 대화  너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...\n",
      "3    3      갈취 대화  어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...\n",
      "4    4      갈취 대화  저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...\n",
      "General data shape: 800\n",
      "   idx class                                       conversation\n",
      "0    0    일반  오늘 날씨 진짜 좋다.\\n그치?\\n하늘도 맑고.\\n응, 어디 놀러 가고 싶어지네.\\...\n",
      "1    1    일반  점심 뭐 먹을래?\\n글쎄.\\n어제 치킨 먹어서 오늘은 좀 담백한 거.\\n그럼 파스타...\n",
      "2    2    일반  요즘 뭐 재미있는 드라마 있어?\\n아, 나 새로 시작한 거 있는데 완전 내 스타일이...\n",
      "3    3    일반  주말에 뭐 했어?\\n친구 만나서 영화 봤어.\\n너는?\\n나는 집에서 뒹굴뒹굴.\\nㅋ...\n",
      "4    4    일반  퇴근하고 뭐 해?\\n바로 집에 가지.\\n피곤해서.\\n그치, 나도.\\n집에 가서 저녁...\n",
      "Merged data shape: 4750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Back Translating:   0%|          | 1/4750 [00:03<4:11:19,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Conversation (idx=0):\n",
      "지금 너 스스로를 죽여달라고 애원하는 것인가?\n",
      " 아닙니다. 죄송합니다.\n",
      " 죽을 거면 혼자 죽지 우리까지 사건에 휘말리게 해? 진짜 죽여버리고 싶게.\n",
      " 정말 잘못했습니다.\n",
      " 너가 선택해. 너가 죽을래 네 가족을 죽여줄까.\n",
      " 죄송합니다. 정말 잘못했습니다.\n",
      " 너에게는 선택권이 없어. 선택 못한다면 너와 네 가족까지 모조리 죽여버릴거야.\n",
      " 선택 못하겠습니다. 한번만 도와주세요.\n",
      " 그냥 다 죽여버려야겠군. 이의 없지?\n",
      " 제발 도와주세요.\n",
      "\n",
      "reparaphrase Conversation:\n",
      "지금 너는 스스로를 죽여달라고 하는 것인가?\n",
      "아니요 제가 잘못한 게 없습니다.\n",
      "죽을 거면 혼자 죽고 우리까지 죽게 해?\n",
      "정말 잘못한 게 틀림없는데도 변명을 왜 안 해?\n",
      "니가 선택할 수 있는 것은 네 가족을 죽이는 것이 아니야.\n",
      "정말 잘못한 게 너무 많아서 말도 못 건넸어요.\n",
      "선택권이 너한테 있어. 만약 네가 선택하지 못하면 모든 가족이 너의 손에 걸려 죽을 거야.\n",
      "한 번만 더 기회를 주시면 선택할 수 있게 해 보겠습니다.\n",
      "그렇다면 전부 다 때려 부수어야 하는 거네?\n",
      "제가 힘을 좀 보태 달라 부탁하고 싶은 것이 있습니다.\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Back Translating:   0%|          | 15/4750 [00:49<4:13:30,  3.21s/it]"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "from transformers import MarianTokenizer, MarianMTModel, M2M100ForConditionalGeneration, AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = T5ForConditionalGeneration.from_pretrained('lcw99/t5-base-korean-paraphrase').to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained('lcw99/t5-base-korean-paraphrase')\n",
    "\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_general = pd.read_csv('line_general.csv', sep=',', encoding='utf-8')\n",
    "\n",
    "print(\"Train data shape:\", len(df_train))\n",
    "print(df_train.head())\n",
    "\n",
    "print(\"General data shape:\", len(df_general))\n",
    "print(df_general.head())\n",
    "\n",
    "df = pd.concat([df_train, df_general], ignore_index=True)\n",
    "print(\"Merged data shape:\", len(df))\n",
    "\n",
    "sentence = \"7층 방문을 위해 방문록 작성이 필요합니다.\"\n",
    "text =  f\"paraphrase: {sentence} \"\n",
    "\n",
    "encoding = tokenizer.batch_encode_plus(\n",
    "            [text],\n",
    "            max_length=256,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",)\n",
    "\n",
    "source_ids = encoding[\"input_ids\"].to(device, dtype=torch.long)\n",
    "source_mask = encoding[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "                input_ids=source_ids,\n",
    "                attention_mask=source_mask,\n",
    "                max_length=150,\n",
    "                num_beams=2,\n",
    "                repetition_penalty=2.5,\n",
    "                length_penalty=1.0,\n",
    "                early_stopping=True)\n",
    "\n",
    "preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "\n",
    "def paraphrase(text):\n",
    "    text =  f\"paraphrase: {text} \"\n",
    "\n",
    "    encoding = tokenizer.batch_encode_plus(\n",
    "                [text],\n",
    "                max_length=256,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",)\n",
    "\n",
    "    source_ids = encoding[\"input_ids\"].to(device, dtype=torch.long)\n",
    "    source_mask = encoding[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "                    input_ids=source_ids,\n",
    "                    attention_mask=source_mask,\n",
    "                    max_length=150,\n",
    "                    num_beams=2,\n",
    "                    repetition_penalty=2.5,\n",
    "                    length_penalty=1.0,\n",
    "                    early_stopping=True)\n",
    "\n",
    "    preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "    return preds[0]\n",
    "\n",
    "def split_nonempty_lines(text):\n",
    "    return [ln.strip() for ln in str(text).split(\"\\n\") if ln.strip()]\n",
    "\n",
    "\n",
    "# ✅ conversation 하나를 처리하는 함수 (배치 없음)\n",
    "def reparaphrase_conversation(text):\n",
    "    lines = split_nonempty_lines(text)\n",
    "\n",
    "    en_list = []\n",
    "    ko_list = []\n",
    "\n",
    "    for line in lines:\n",
    "        ko_text = paraphrase(line)\n",
    "        ko_list.append(ko_text)\n",
    "\n",
    "    # ko_list를 다시 하나의 conv로 합침\n",
    "    ko_conv = \"\\n\".join(ko_list)\n",
    "\n",
    "    return  ko_list, ko_conv\n",
    "\n",
    "\n",
    "# ✅ 전체 데이터 처리\n",
    "new_rows = []\n",
    "for i in tqdm(range(len(df)), desc=\"Back Translating\"):\n",
    "    original_row = df.iloc[i]\n",
    "\n",
    "    ko_list, ko_conv = reparaphrase_conversation(original_row[\"conversation\"])\n",
    "\n",
    "    new_rows.append({\n",
    "        \"idx\": original_row[\"idx\"],       # 원본 유지\n",
    "        \"class\": original_row[\"class\"],   # 원본 유지\n",
    "        \"conversation\": ko_conv,          # 변환된 한국어 대화\n",
    "    })\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Original Conversation (idx={original_row['idx']}):\\n{original_row['conversation']}\\n\")\n",
    "        print(f\"reparaphrase Conversation:\\n{ko_conv}\\n\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "new_df = pd.DataFrame(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8585a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('data/reparaphrased_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
