{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68bc88cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\C\\anaconda3\\envs\\jedi\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (357 > 300). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1/3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/KcBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[baseline] ep01 loss=0.7468 f1=0.9002 (use_len=300)\n",
      "[baseline] ep02 loss=0.2051 f1=0.9103 (use_len=300)\n",
      "[baseline] ep03 loss=0.0818 f1=0.9180 (use_len=300)\n",
      "[baseline] ep04 loss=0.0373 f1=0.9329 (use_len=300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/KcBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 276\u001b[39m\n\u001b[32m    273\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mΔ mean=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiff.mean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 249\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    246\u001b[39m f1_a, rep_a, cm_a = run_fold(X_tr, y_tr, X_va, y_va, label_names,\n\u001b[32m    247\u001b[39m                              loss_mode=\u001b[33m\"\u001b[39m\u001b[33mbaseline\u001b[39m\u001b[33m\"\u001b[39m, global_tokenizer=global_tok, kw_map=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# (B) 키워드가중 CE\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m f1_b, rep_b, cm_b = \u001b[43mrun_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_va\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_va\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mloss_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweighted\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_tokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mglobal_tok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkw_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m results[\u001b[33m\"\u001b[39m\u001b[33mbaseline\u001b[39m\u001b[33m\"\u001b[39m].append({\u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m: f1_a, \u001b[33m\"\u001b[39m\u001b[33mreport\u001b[39m\u001b[33m\"\u001b[39m: rep_a, \u001b[33m\"\u001b[39m\u001b[33mcm\u001b[39m\u001b[33m\"\u001b[39m: cm_a.tolist()})\n\u001b[32m    253\u001b[39m results[\u001b[33m\"\u001b[39m\u001b[33mweighted\u001b[39m\u001b[33m\"\u001b[39m].append({\u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m: f1_b, \u001b[33m\"\u001b[39m\u001b[33mreport\u001b[39m\u001b[33m\"\u001b[39m: rep_b, \u001b[33m\"\u001b[39m\u001b[33mcm\u001b[39m\u001b[33m\"\u001b[39m: cm_b.tolist()})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 210\u001b[39m, in \u001b[36mrun_fold\u001b[39m\u001b[34m(X_tr, y_tr, X_va, y_va, label_names, loss_mode, global_tokenizer, kw_map)\u001b[39m\n\u001b[32m    208\u001b[39m best_f1, best_rep, best_cm = -\u001b[32m1.0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, CFG.epochs + \u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     tr_loss = \u001b[43mtrain_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m     f1, rep, cm = evaluate(model, va_loader, label_names)\n\u001b[32m    212\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] ep\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m f1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (use_len=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muse_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 155\u001b[39m, in \u001b[36mtrain_one\u001b[39m\u001b[34m(model, loader, optimizer, scheduler, loss_mode, tokenizer, kw_map)\u001b[39m\n\u001b[32m    153\u001b[39m out = model(**enc)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loss_mode == \u001b[33m\"\u001b[39m\u001b[33mweighted\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     loss = \u001b[43mloss_ce_token_weighted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCFG\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoken_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    157\u001b[39m     loss = loss_ce(out.logits, y)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 130\u001b[39m, in \u001b[36mloss_ce_token_weighted\u001b[39m\u001b[34m(logits, labels, input_ids, tokenizer, kw_map, weight)\u001b[39m\n\u001b[32m    127\u001b[39m base = F.cross_entropy(logits, labels, reduction=\u001b[33m\"\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# 클래스별 키토큰 ID 집합\u001b[39;00m\n\u001b[32m    129\u001b[39m cls_token_ids = {\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     c: \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_tokens_to_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtok\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m c, toks \u001b[38;5;129;01min\u001b[39;00m kw_map.items()\n\u001b[32m    132\u001b[39m }\n\u001b[32m    133\u001b[39m B = input_ids.size(\u001b[32m0\u001b[39m)\n\u001b[32m    134\u001b[39m mult = torch.ones(B, device=logits.device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 130\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    127\u001b[39m base = F.cross_entropy(logits, labels, reduction=\u001b[33m\"\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# 클래스별 키토큰 ID 집합\u001b[39;00m\n\u001b[32m    129\u001b[39m cls_token_ids = {\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     c: \u001b[38;5;28mset\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_tokens_to_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtok\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m toks \u001b[38;5;28;01mif\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m tokenizer.vocab)\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m c, toks \u001b[38;5;129;01min\u001b[39;00m kw_map.items()\n\u001b[32m    132\u001b[39m }\n\u001b[32m    133\u001b[39m B = input_ids.size(\u001b[32m0\u001b[39m)\n\u001b[32m    134\u001b[39m mult = torch.ones(B, device=logits.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\C\\anaconda3\\envs\\jedi\\Lib\\site-packages\\transformers\\tokenization_utils_fast.py:334\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.convert_tokens_to_ids\u001b[39m\u001b[34m(self, tokens)\u001b[39m\n\u001b[32m    330\u001b[39m             encoding_dict[\u001b[33m\"\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m\"\u001b[39m].append(\u001b[38;5;28mlen\u001b[39m(e.ids))\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m encoding_dict, encodings\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert_tokens_to_ids\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]]) -> Union[\u001b[38;5;28mint\u001b[39m, List[\u001b[38;5;28mint\u001b[39m]]:\n\u001b[32m    335\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    336\u001b[39m \u001b[33;03m    Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[33;03m    vocabulary.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    343\u001b[39m \u001b[33;03m        `int` or `List[int]`: The token id or list of token ids.\u001b[39;00m\n\u001b[32m    344\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    345\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# analyze_keyword_weighting_fixed.py\n",
    "# 목적: 클래스별 키워드 가중치(loss)에 따른 성능 향상 여부 검증\n",
    "# 비교: (A) 기본 CrossEntropy, (B) 키워드가중 CrossEntropy\n",
    "# 지표: weighted F1\n",
    "# 입력: data/train.csv  (columns: conversation, class)\n",
    "\n",
    "import os, json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup, set_seed\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# 설정\n",
    "# -----------------------\n",
    "@dataclass\n",
    "class CFG:\n",
    "    model_name: str = \"beomi/KcBERT-base\"  # 필요시 교체\n",
    "    data_path: str = \"data/train.csv\"\n",
    "    text_col: str = \"conversation\"\n",
    "    label_col: str = \"class\"\n",
    "    max_len: int = 512           # 상한. 모델 max_position_embeddings와 min 적용\n",
    "    batch_size: int = 16\n",
    "    lr: float = 2e-5\n",
    "    epochs: int = 4\n",
    "    warmup_ratio: float = 0.1\n",
    "    seed: int = 2025\n",
    "    kfold: int = 3\n",
    "    out_dir: str = \"kw_weighting_results\"\n",
    "    # 키워드 가중치 세팅\n",
    "    token_weight: float = 1.25   # 발견 시 배 가중\n",
    "    min_df: int = 3              # TF-IDF 최소 등장 문서 수\n",
    "    topk_per_class: int = 50     # 클래스별 상위 토큰 수\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.makedirs(CFG.out_dir, exist_ok=True)\n",
    "set_seed(CFG.seed)\n",
    "\n",
    "# -----------------------\n",
    "# 데이터 / 라벨 매핑\n",
    "# -----------------------\n",
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    assert CFG.text_col in df.columns and CFG.label_col in df.columns\n",
    "    df = df[[CFG.text_col, CFG.label_col]].dropna().reset_index(drop=True)\n",
    "    df[CFG.text_col] = df[CFG.text_col].astype(str)\n",
    "    df[CFG.label_col] = df[CFG.label_col].astype(str)\n",
    "    return df\n",
    "\n",
    "def build_label_maps(labels: List[str]) -> Tuple[List[str], Dict[str,int], Dict[int,str]]:\n",
    "    uniq = sorted(list(set(labels)))\n",
    "    lid = {l:i for i,l in enumerate(uniq)}\n",
    "    inv = {i:l for l,i in lid.items()}\n",
    "    return uniq, lid, inv\n",
    "\n",
    "# -----------------------\n",
    "# 데이터셋 / 콜레이트\n",
    "# -----------------------\n",
    "class RawDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.texts, self.labels = texts, labels\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, i):\n",
    "        y = -1 if self.labels is None else self.labels[i]\n",
    "        return self.texts[i], y\n",
    "\n",
    "def make_collate(tokenizer, max_len: int):\n",
    "    def _fn(batch):\n",
    "        xs, ys = zip(*batch)\n",
    "        enc = tokenizer(\n",
    "            list(xs),\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"],\n",
    "            \"attention_mask\": enc[\"attention_mask\"]\n",
    "        }, torch.tensor(ys, dtype=torch.long)\n",
    "    return _fn\n",
    "\n",
    "# -----------------------\n",
    "# 키워드 사전 생성 (TF-IDF, 토큰 단위)\n",
    "# -----------------------\n",
    "def extract_class_keywords(df: pd.DataFrame, labels_order: List[str], tokenizer) -> Dict[int, List[str]]:\n",
    "    texts_tok = []\n",
    "    for t in df[CFG.text_col].tolist():\n",
    "        toks = tokenizer.tokenize(t)[:CFG.max_len - 2]\n",
    "        texts_tok.append(\" \".join(toks))\n",
    "\n",
    "    tfidf = TfidfVectorizer(min_df=CFG.min_df, token_pattern=r\"[^ ]+\")\n",
    "    X = tfidf.fit_transform(texts_tok)\n",
    "    vocab = np.array(tfidf.get_feature_names_out())\n",
    "\n",
    "    kw = {}\n",
    "    y = df[CFG.label_col].values\n",
    "    for li, lab in enumerate(labels_order):\n",
    "        mask = (y == lab)\n",
    "        if mask.sum() == 0:\n",
    "            kw[li] = []\n",
    "            continue\n",
    "        mean_tfidf = X[mask].mean(axis=0).A1\n",
    "        topk_idx = np.argsort(-mean_tfidf)[:CFG.topk_per_class]\n",
    "        kw_tokens = vocab[topk_idx].tolist()\n",
    "        kw[li] = kw_tokens\n",
    "    return kw\n",
    "\n",
    "# -----------------------\n",
    "# 손실 함수\n",
    "# -----------------------\n",
    "def loss_ce(logits, labels):\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "def loss_ce_token_weighted(logits, labels, input_ids, tokenizer, kw_map: Dict[int, List[str]], weight: float):\n",
    "    base = F.cross_entropy(logits, labels, reduction=\"none\")\n",
    "    # 클래스별 키토큰 ID 집합\n",
    "    cls_token_ids = {\n",
    "        c: set(tokenizer.convert_tokens_to_ids(tok) for tok in toks if tok in tokenizer.vocab)\n",
    "        for c, toks in kw_map.items()\n",
    "    }\n",
    "    B = input_ids.size(0)\n",
    "    mult = torch.ones(B, device=logits.device)\n",
    "    for cls_idx, idset in cls_token_ids.items():\n",
    "        if not idset:\n",
    "            continue\n",
    "        id_tensor = torch.tensor(list(idset), device=input_ids.device)\n",
    "        has_key = torch.isin(input_ids, id_tensor).any(dim=1)\n",
    "        mask = (labels == cls_idx) & has_key\n",
    "        mult = torch.where(mask, mult * weight, mult)\n",
    "    return (base * mult).mean()\n",
    "\n",
    "# -----------------------\n",
    "# 학습/평가 루틴\n",
    "# -----------------------\n",
    "def train_one(model, loader, optimizer, scheduler, loss_mode, tokenizer, kw_map=None):\n",
    "    model.train()\n",
    "    tot = 0.0\n",
    "    for enc, y in loader:\n",
    "        enc = {k:v.to(device) for k,v in enc.items()}\n",
    "        y = y.to(device)\n",
    "        out = model(**enc)\n",
    "        if loss_mode == \"weighted\":\n",
    "            loss = loss_ce_token_weighted(out.logits, y, enc[\"input_ids\"], tokenizer, kw_map, CFG.token_weight)\n",
    "        else:\n",
    "            loss = loss_ce(out.logits, y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        tot += loss.item()\n",
    "    return tot / max(1, len(loader))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, label_names):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    for enc, y in loader:\n",
    "        enc = {k:v.to(device) for k,v in enc.items()}\n",
    "        y = y.to(device)\n",
    "        logits = model(**enc).logits\n",
    "        pred = logits.argmax(1)\n",
    "        ys.append(y.cpu()); ps.append(pred.cpu())\n",
    "    y_true = torch.cat(ys).numpy()\n",
    "    y_pred = torch.cat(ps).numpy()\n",
    "    # weighted F1\n",
    "    overall_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    rep = classification_report(y_true, y_pred, target_names=label_names, digits=4, output_dict=True)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return overall_f1, rep, cm\n",
    "\n",
    "def run_fold(X_tr, y_tr, X_va, y_va, label_names, loss_mode, global_tokenizer, kw_map):\n",
    "    # 모델 로드\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        CFG.model_name, num_labels=len(label_names)\n",
    "    ).to(device)\n",
    "\n",
    "    # 모델 최대 길이와 동기화\n",
    "    model_max = int(getattr(model.config, \"max_position_embeddings\", CFG.max_len))\n",
    "    use_len = min(CFG.max_len, model_max)\n",
    "    # 토크나이저 길이도 맞춤\n",
    "    tokenizer = global_tokenizer\n",
    "    tokenizer.model_max_length = use_len\n",
    "\n",
    "    # 데이터로더\n",
    "    train_ds = RawDataset(X_tr, y_tr)\n",
    "    val_ds   = RawDataset(X_va, y_va)\n",
    "    collate  = make_collate(tokenizer, use_len)\n",
    "    tr_loader = DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True,  collate_fn=collate)\n",
    "    va_loader = DataLoader(val_ds,   batch_size=CFG.batch_size, shuffle=False, collate_fn=collate)\n",
    "\n",
    "    steps = len(tr_loader) * CFG.epochs\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=0.01)\n",
    "    sch = get_linear_schedule_with_warmup(opt, int(CFG.warmup_ratio * steps), steps)\n",
    "\n",
    "    best_f1, best_rep, best_cm = -1.0, None, None\n",
    "    for ep in range(1, CFG.epochs + 1):\n",
    "        tr_loss = train_one(model, tr_loader, opt, sch, loss_mode, tokenizer, kw_map)\n",
    "        f1, rep, cm = evaluate(model, va_loader, label_names)\n",
    "        print(f\"[{loss_mode}] ep{ep:02d} loss={tr_loss:.4f} f1={f1:.4f} (use_len={use_len})\")\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_rep, best_cm = f1, rep, cm\n",
    "    return best_f1, best_rep, best_cm\n",
    "\n",
    "# -----------------------\n",
    "# 메인\n",
    "# -----------------------\n",
    "def main():\n",
    "    df = load_data(CFG.data_path)\n",
    "    label_names, lid, inv = build_label_maps(df[CFG.label_col].tolist())\n",
    "    y = np.array([lid[v] for v in df[CFG.label_col].tolist()])\n",
    "    X = df[CFG.text_col].tolist()\n",
    "\n",
    "    # 토크나이저(공유). 길이는 fold에서 모델과 동기화.\n",
    "    global_tok = AutoTokenizer.from_pretrained(CFG.model_name, use_fast=True)\n",
    "\n",
    "    # 클래스별 키워드 자동 추출 (토큰 단위)\n",
    "    # 주의: tokenizer.tokenize 결과를 사용하므로 모델 교체시 재실행 권장\n",
    "    kw_map = extract_class_keywords(df, label_names, global_tok)\n",
    "    with open(os.path.join(CFG.out_dir, \"class_keywords.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({label_names[k]: v for k,v in kw_map.items()}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=CFG.kfold, shuffle=True, random_state=CFG.seed)\n",
    "    results = {\"baseline\": [], \"weighted\": []}\n",
    "\n",
    "    fold_id = 0\n",
    "    for tr_idx, va_idx in kf.split(np.arange(len(X)), y):\n",
    "        fold_id += 1\n",
    "        print(f\"\\n===== Fold {fold_id}/{CFG.kfold} =====\")\n",
    "        X_tr = [X[i] for i in tr_idx]; y_tr = y[tr_idx].tolist()\n",
    "        X_va = [X[i] for i in va_idx]; y_va = y[va_idx].tolist()\n",
    "\n",
    "        # (A) 기본 CE\n",
    "        f1_a, rep_a, cm_a = run_fold(X_tr, y_tr, X_va, y_va, label_names,\n",
    "                                     loss_mode=\"baseline\", global_tokenizer=global_tok, kw_map=None)\n",
    "        # (B) 키워드가중 CE\n",
    "        f1_b, rep_b, cm_b = run_fold(X_tr, y_tr, X_va, y_va, label_names,\n",
    "                                     loss_mode=\"weighted\", global_tokenizer=global_tok, kw_map=kw_map)\n",
    "\n",
    "        results[\"baseline\"].append({\"f1\": f1_a, \"report\": rep_a, \"cm\": cm_a.tolist()})\n",
    "        results[\"weighted\"].append({\"f1\": f1_b, \"report\": rep_b, \"cm\": cm_b.tolist()})\n",
    "\n",
    "    # 요약\n",
    "    def summarize(key):\n",
    "        vals = [r[\"f1\"] for r in results[key]]\n",
    "        return {\"f1_mean\": float(np.mean(vals)), \"f1_std\": float(np.std(vals))}\n",
    "    summ = {k: summarize(k) for k in results.keys()}\n",
    "\n",
    "    print(\"\\n=== Summary (weighted F1) ===\")\n",
    "    print(\"Baseline :\", summ[\"baseline\"])\n",
    "    print(\"Weighted :\", summ[\"weighted\"])\n",
    "\n",
    "    with open(os.path.join(CFG.out_dir, \"summary.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"summary\": summ, \"details\": results}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 각 fold 차이\n",
    "    b = np.array([r[\"f1\"] for r in results[\"baseline\"]])\n",
    "    w = np.array([r[\"f1\"] for r in results[\"weighted\"]])\n",
    "    diff = w - b\n",
    "    print(f\"Per-fold ΔF1 (weighted - baseline): {diff.tolist()}\")\n",
    "    print(f\"Δ mean={diff.mean():.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jedi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
