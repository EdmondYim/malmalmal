{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7be1480a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸš€ CNN Baseline: bertcnn\n",
      "============================================================\n",
      "Model: beomi/KcBERT-base\n",
      "Max length: 384\n",
      "Batch size: 16\n",
      "Epochs: 10\n",
      "Class weight: True\n",
      "============================================================\n",
      "\n",
      "Train samples: 4750\n",
      "Test samples: 500\n",
      "\n",
      "Classes: 5\n",
      "  00: í˜‘ë°• ëŒ€í™” (896ê°œ)\n",
      "  01: ê°ˆì·¨ ëŒ€í™” (981ê°œ)\n",
      "  02: ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™” (979ê°œ)\n",
      "  03: ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™” (1094ê°œ)\n",
      "  04: ì¼ë°˜ (800ê°œ)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\C\\anaconda3\\envs\\jedi\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer max_len: 300\n",
      "\n",
      "Class weights:\n",
      "  00 í˜‘ë°• ëŒ€í™”: 1.2723\n",
      "  01 ê°ˆì·¨ ëŒ€í™”: 0.9684\n",
      "  02 ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”: 0.9704\n",
      "  03 ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”: 1.1289\n",
      "  04 ì¼ë°˜: 1.1875\n",
      "\n",
      "Training started!\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/10  loss=1.2516  val_f1=0.9041\n",
      "  âœ… Best model saved! (F1: 0.9041)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2/10  loss=0.3215  val_f1=0.9137\n",
      "  âœ… Best model saved! (F1: 0.9137)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3/10  loss=0.1704  val_f1=0.9353\n",
      "  âœ… Best model saved! (F1: 0.9353)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4/10  loss=0.0698  val_f1=0.9246\n",
      "  No improvement (1/4)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5/10  loss=0.0239  val_f1=0.9308\n",
      "  No improvement (2/4)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6/10  loss=0.0046  val_f1=0.9314\n",
      "  No improvement (3/4)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7/10  loss=0.0033  val_f1=0.9355\n",
      "  âœ… Best model saved! (F1: 0.9355)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8/10  loss=0.0009  val_f1=0.9408\n",
      "  âœ… Best model saved! (F1: 0.9408)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9/10  loss=0.0006  val_f1=0.9422\n",
      "  âœ… Best model saved! (F1: 0.9422)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10  loss=0.0001  val_f1=0.9449\n",
      "  âœ… Best model saved! (F1: 0.9449)\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š Final Report:\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       í˜‘ë°• ëŒ€í™”     0.9265    0.9333    0.9299       135\n",
      "       ê°ˆì·¨ ëŒ€í™”     0.9060    0.9184    0.9122       147\n",
      " ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”     0.9655    0.9524    0.9589       147\n",
      "   ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”     0.9264    0.9207    0.9235       164\n",
      "          ì¼ë°˜     1.0000    1.0000    1.0000       120\n",
      "\n",
      "    accuracy                         0.9425       713\n",
      "   macro avg     0.9449    0.9450    0.9449       713\n",
      "weighted avg     0.9427    0.9425    0.9426       713\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:01<00:00, 18.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Submission saved: outputs\\submission_bertcnn.csv\n",
      "Shape: (500, 2)\n",
      "\n",
      "Sample predictions:\n",
      "     idx class\n",
      "0  t_000    01\n",
      "1  t_001    02\n",
      "2  t_002    02\n",
      "3  t_003    04\n",
      "4  t_004    04\n"
     ]
    }
   ],
   "source": [
    "# run_cnn_baselines.py (ìˆ˜ì • ë²„ì „)\n",
    "# PyTorch CNN baselines for text classification\n",
    "# Fix: masking, submission format, preprocessing consistency\n",
    "\n",
    "import os, math, random, numpy as np, pandas as pd, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoConfig, set_seed\n",
    ")\n",
    "\n",
    "# ---------------- CFG ----------------\n",
    "class CFG:\n",
    "    model_type   = \"bertcnn\"          # \"bertcnn\" or \"textcnn\"\n",
    "    bert_name    = \"beomi/KcBERT-base\"  # ğŸ”¥ ìˆ˜ì •: KcBERT \n",
    "    max_len      = 384                # ğŸ”¥ ìˆ˜ì •: 512 â†’ 384 (ì•ˆì •ì )\n",
    "    batch_size   = 16\n",
    "    epochs       = 10                 # ğŸ”¥ ìˆ˜ì •: 5 â†’ 10\n",
    "    lr           = 2e-5\n",
    "    lr_textcnn   = 1e-3\n",
    "    weight_decay = 0.01\n",
    "    val_ratio    = 0.15\n",
    "    seed         = 2025\n",
    "    data_dir     = \"data\"\n",
    "    out_dir      = \"outputs\"\n",
    "    device       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # ğŸ”¥ ì¶”ê°€ ì˜µì…˜\n",
    "    use_class_weight = True           # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜\n",
    "    early_stopping_patience = 4\n",
    "\n",
    "os.makedirs(CFG.out_dir, exist_ok=True)\n",
    "set_seed(CFG.seed)\n",
    "device = torch.device(CFG.device)\n",
    "\n",
    "\n",
    "# --------- IO ----------\n",
    "def load_train_df(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.rename(columns={\"conversation\":\"text\",\"class\":\"label\"})\n",
    "    \n",
    "    \n",
    "    df[\"text\"] = df[\"text\"].astype(str).str.replace(\"\\n\", \"[SEP]\")\n",
    "    df[\"text\"] = df[\"text\"].str.replace(r'\\s+', ' ', regex=True)  # ì—°ì† ê³µë°± ì œê±°\n",
    "    df[\"text\"] = df[\"text\"].str.strip()\n",
    "    \n",
    "    df[\"label\"] = df[\"label\"].astype(str)\n",
    "    return df\n",
    "\n",
    "def load_test_df(path):\n",
    "    df = pd.read_csv(path)\n",
    "    # ì›ë³¸ ë³´ì¡´ (ì œì¶œìš©)\n",
    "    df[\"text_original\"] = df[\"conversation\"].astype(str)\n",
    "    \n",
    "    # ëª¨ë¸ ì…ë ¥ìš© ì „ì²˜ë¦¬ (trainê³¼ ë™ì¼)\n",
    "    df[\"text\"] = df[\"conversation\"].astype(str).str.replace(\"\\n\", \" \")\n",
    "    df[\"text\"] = df[\"text\"].str.replace(r'\\s+', ' ', regex=True)\n",
    "    df[\"text\"] = df[\"text\"].str.strip()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ì œì¶œ ì½”ë“œ ë§¤í•‘\n",
    "NAME2CODE = {\n",
    "    'í˜‘ë°• ëŒ€í™”': '00',\n",
    "    'ê°ˆì·¨ ëŒ€í™”': '01',\n",
    "    'ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”': '02',\n",
    "    'ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”': '03',\n",
    "    'ì¼ë°˜ ëŒ€í™”': '04',\n",
    "}\n",
    "\n",
    "# -------- Tokenizer & Collate --------\n",
    "def prepare_tokenizer_and_maxlen():\n",
    "    cfg = AutoConfig.from_pretrained(CFG.bert_name)\n",
    "    model_limit = getattr(cfg, \"max_position_embeddings\", 512)\n",
    "    CFG.max_len = min(CFG.max_len, model_limit)\n",
    "    \n",
    "    tok = AutoTokenizer.from_pretrained(CFG.bert_name, use_fast=True)\n",
    "    tok.model_max_length = CFG.max_len\n",
    "    return tok\n",
    "\n",
    "def make_collate(tokenizer):\n",
    "    def _fn(batch):\n",
    "        texts, labels = zip(*batch)\n",
    "        enc = tokenizer(\n",
    "            list(texts),\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=CFG.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        y = torch.tensor(labels, dtype=torch.long)\n",
    "        return {\"input_ids\": enc[\"input_ids\"], \"attention_mask\": enc[\"attention_mask\"]}, y\n",
    "    return _fn\n",
    "\n",
    "class RawDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, i):\n",
    "        y = -1 if self.labels is None else self.labels[i]\n",
    "        return self.texts[i], y\n",
    "\n",
    "# -------- Models (ìˆ˜ì • ë²„ì „) --------\n",
    "class BertCnnHeadV2(nn.Module):\n",
    "    \"\"\"BERT + CNN Head (ê°œì„  ë²„ì „)\"\"\"\n",
    "    def __init__(self, bert_name, num_labels, kernels=(2,3,4), channels=128, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(bert_name)\n",
    "        hid = self.backbone.config.hidden_size\n",
    "        \n",
    "        # ğŸ”¥ Same paddingìœ¼ë¡œ ë³€ê²½\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(hid, channels, k, padding=k//2)\n",
    "            for k in kernels\n",
    "        ])\n",
    "        self.batch_norms = nn.ModuleList([\n",
    "            nn.BatchNorm1d(channels) for _ in kernels\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(channels * len(kernels), num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        H = out.last_hidden_state              # [B, L, H]\n",
    "        \n",
    "        # ğŸ”¥ ê°œì„ : Attention mask ì§ì ‘ ê³±í•˜ê¸°\n",
    "        H = H * attention_mask.unsqueeze(-1)   # Padding ì œê±°\n",
    "        \n",
    "        x = H.transpose(1, 2)                  # [B, H, L]\n",
    "        \n",
    "        feats = []\n",
    "        for conv, bn in zip(self.convs, self.batch_norms):\n",
    "            h = conv(x)                        # [B, C, L]\n",
    "            h = bn(h)\n",
    "            h = F.relu(h)\n",
    "            # ğŸ”¥ ê°„ë‹¨í•œ Max Pooling (masking ë¶ˆí•„ìš”)\n",
    "            h = torch.max(h, dim=-1).values    # [B, C]\n",
    "            feats.append(h)\n",
    "        \n",
    "        z = torch.cat(feats, dim=1)\n",
    "        z = self.dropout(z)\n",
    "        return self.fc(z)\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    \"\"\"TextCNN (ì›ë³¸ ìœ ì§€)\"\"\"\n",
    "    def __init__(self, vocab_size, pad_id, num_labels, emb_dim=300, kernels=(3,4,5), channels=128, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_id)\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(emb_dim, channels, k) for k in kernels])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(channels * len(kernels), num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.emb(input_ids)              # [B, L, E]\n",
    "        x = x.transpose(1, 2)                # [B, E, L]\n",
    "        feats = []\n",
    "        for conv in self.convs:\n",
    "            h = F.relu(conv(x))              # [B, C, L']\n",
    "            h = torch.max(h, dim=-1).values  # [B, C]\n",
    "            feats.append(h)\n",
    "        z = torch.cat(feats, dim=1)\n",
    "        z = self.dropout(z)\n",
    "        return self.fc(z)\n",
    "\n",
    "# -------- Class Weight --------\n",
    "def compute_class_weights(y_all, num_classes):\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.arange(num_classes),\n",
    "        y=y_all\n",
    "    )\n",
    "    # ğŸ”¥ ê¸°íƒ€ ê´´ë¡­í˜(3)ê³¼ í˜‘ë°•(0) ë¶€ìŠ¤íŠ¸\n",
    "    class_weights[3] *= 1.3\n",
    "    class_weights[0] *= 1.2\n",
    "    return torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "# -------- Train/Eval --------\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    for enc, y in loader:\n",
    "        enc = {k:v.to(device) for k,v in enc.items()}\n",
    "        y = y.to(device)\n",
    "        logits = model(**enc)\n",
    "        ps.append(logits.softmax(1).cpu())\n",
    "        ys.append(y.cpu())\n",
    "    P = torch.cat(ps).numpy()\n",
    "    Y = torch.cat(ys).numpy()\n",
    "    preds = P.argmax(1)\n",
    "    f1 = f1_score(Y, preds, average=\"macro\")\n",
    "    return f1, Y, preds\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, scheduler=None, class_weights=None):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    pbar = tqdm(loader, ncols=100, leave=False, desc=\"Training\")\n",
    "    for enc, y in pbar:\n",
    "        enc = {k:v.to(device) for k,v in enc.items()}\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(**enc)\n",
    "        \n",
    "        # ğŸ”¥ Class weight ì ìš©\n",
    "        if class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, y, weight=class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        if scheduler: scheduler.step()\n",
    "        total += loss.item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total / max(1, len(loader))\n",
    "\n",
    "# -------- Main --------\n",
    "def main():\n",
    "    print(\"=\"*60)\n",
    "    print(f\"ğŸš€ CNN Baseline: {CFG.model_type}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Model: {CFG.bert_name}\")\n",
    "    print(f\"Max length: {CFG.max_len}\")\n",
    "    print(f\"Batch size: {CFG.batch_size}\")\n",
    "    print(f\"Epochs: {CFG.epochs}\")\n",
    "    print(f\"Class weight: {CFG.use_class_weight}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # 1) Data\n",
    "    train_df = load_train_df(os.path.join(CFG.data_dir, \"strage_1.csv\"))\n",
    "    test_df  = load_test_df(os.path.join(CFG.data_dir, \"test.csv\"))\n",
    "    \n",
    "    print(f\"Train samples: {len(train_df)}\")\n",
    "    print(f\"Test samples: {len(test_df)}\\n\")\n",
    "\n",
    "    # labels (ê³ ì • ìˆœì„œ)\n",
    "    def order_key(name):\n",
    "        return list(NAME2CODE.keys()).index(name) if name in NAME2CODE else 999\n",
    "    labels = sorted(train_df[\"label\"].unique().tolist(), key=order_key)\n",
    "    lid = {l:i for i,l in enumerate(labels)}\n",
    "    id2code = {i: NAME2CODE.get(labels[i], f\"{i:02d}\") for i in range(len(labels))}\n",
    "    \n",
    "    print(f\"Classes: {len(labels)}\")\n",
    "    for i, label in enumerate(labels):\n",
    "        count = (train_df[\"label\"] == label).sum()\n",
    "        print(f\"  {i:02d}: {label} ({count}ê°œ)\")\n",
    "    print()\n",
    "    \n",
    "    y_all = np.array([lid[l] for l in train_df[\"label\"].tolist()])\n",
    "    texts = train_df[\"text\"].tolist()\n",
    "\n",
    "    # 2) tokenizer\n",
    "    tok = prepare_tokenizer_and_maxlen()\n",
    "    print(f\"Tokenizer max_len: {CFG.max_len}\\n\")\n",
    "\n",
    "    # 3) stratified split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=CFG.val_ratio, random_state=CFG.seed)\n",
    "    tr_idx, va_idx = next(sss.split(np.arange(len(texts)), y_all))\n",
    "    tr_texts, va_texts = [texts[i] for i in tr_idx], [texts[i] for i in va_idx]\n",
    "    tr_labels, va_labels = y_all[tr_idx].tolist(), y_all[va_idx].tolist()\n",
    "\n",
    "    # 4) datasets/loaders\n",
    "    collate = make_collate(tok)\n",
    "    train_ds = RawDataset(tr_texts, tr_labels)\n",
    "    val_ds   = RawDataset(va_texts, va_labels)\n",
    "    test_ds  = RawDataset(test_df[\"text\"].tolist(), None)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True,  collate_fn=collate, num_workers=0)\n",
    "    val_loader   = torch.utils.data.DataLoader(val_ds,   batch_size=CFG.batch_size, shuffle=False, collate_fn=collate, num_workers=0)\n",
    "    test_loader  = torch.utils.data.DataLoader(test_ds,  batch_size=CFG.batch_size, shuffle=False, collate_fn=collate, num_workers=0)\n",
    "\n",
    "    # 5) model\n",
    "    if CFG.model_type == \"textcnn\":\n",
    "        model = TextCNN(\n",
    "            vocab_size=tok.vocab_size,\n",
    "            pad_id=tok.pad_token_id if tok.pad_token_id is not None else 0,\n",
    "            num_labels=len(labels),\n",
    "            emb_dim=300, kernels=(3,4,5), channels=128, dropout=0.2\n",
    "        )\n",
    "        lr = CFG.lr_textcnn\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "        scheduler = None\n",
    "        class_weights = None\n",
    "    else:\n",
    "        # ğŸ”¥ ìˆ˜ì •: BertCnnHeadV2 ì‚¬ìš©\n",
    "        model = BertCnnHeadV2(\n",
    "            CFG.bert_name, \n",
    "            num_labels=len(labels), \n",
    "            kernels=(2,3,4), \n",
    "            channels=128, \n",
    "            dropout=0.3\n",
    "        )\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        grouped = [\n",
    "            {\"params\":[p for n,p in model.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\":CFG.weight_decay},\n",
    "            {\"params\":[p for n,p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\":0.0},\n",
    "        ]\n",
    "        lr = CFG.lr\n",
    "        optimizer = torch.optim.AdamW(grouped, lr=lr)\n",
    "        total_steps = math.ceil(len(train_loader) * CFG.epochs)\n",
    "        warmup = max(1, int(0.1 * total_steps))\n",
    "        scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "            optimizer,\n",
    "            schedulers=[\n",
    "                torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, total_iters=warmup),\n",
    "                torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=total_steps - warmup),\n",
    "            ],\n",
    "            milestones=[warmup]\n",
    "        )\n",
    "        \n",
    "        # ğŸ”¥ ì¶”ê°€: Class weights\n",
    "        if CFG.use_class_weight:\n",
    "            class_weights = compute_class_weights(y_all, len(labels))\n",
    "            print(\"Class weights:\")\n",
    "            for i, (label, weight) in enumerate(zip(labels, class_weights)):\n",
    "                print(f\"  {i:02d} {label}: {weight:.4f}\")\n",
    "            print()\n",
    "        else:\n",
    "            class_weights = None\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 6) train\n",
    "    print(\"Training started!\\n\")\n",
    "    print(\"-\"*60)\n",
    "    best_f1, best_state = -1.0, None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for ep in range(1, CFG.epochs+1):\n",
    "        tr_loss = train_one_epoch(model, train_loader, optimizer, scheduler, class_weights)\n",
    "        f1, y_true, y_pred = evaluate(model, val_loader)\n",
    "        \n",
    "        print(f\"Epoch {ep:2d}/{CFG.epochs}  loss={tr_loss:.4f}  val_f1={f1:.4f}\")\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_state = f1, {k:v.cpu().clone() for k,v in model.state_dict().items()}\n",
    "            report = classification_report(y_true, y_pred, target_names=labels, digits=4)\n",
    "            print(f\"  âœ… Best model saved! (F1: {best_f1:.4f})\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement ({patience_counter}/{CFG.early_stopping_patience})\")\n",
    "        \n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        if patience_counter >= CFG.early_stopping_patience:\n",
    "            print(\"\\nâš ï¸ Early stopping!\")\n",
    "            break\n",
    "\n",
    "    # 7) save and predict\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“Š Final Report:\")\n",
    "    print(\"=\"*60)\n",
    "    print(report)\n",
    "    \n",
    "    torch.save({\"state_dict\": best_state, \"labels\": labels}, os.path.join(CFG.out_dir, f\"{CFG.model_type}_best.pt\"))\n",
    "    with open(os.path.join(CFG.out_dir, f\"{CFG.model_type}_val_report.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "    # load best to predict\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for enc, _ in tqdm(test_loader, ncols=100, desc=\"Predicting\"):\n",
    "            enc = {k:v.to(device) for k,v in enc.items()}\n",
    "            logits = model(**enc)\n",
    "            preds.append(logits.argmax(1).cpu())\n",
    "    \n",
    "    pred_ids = torch.cat(preds).numpy()\n",
    "    \n",
    "    # ğŸ”¥ ìˆ˜ì •: ì˜ˆì¸¡ ID â†’ í´ë˜ìŠ¤ ì´ë¦„ â†’ ì½”ë“œ\n",
    "    pred_codes = [NAME2CODE.get(labels[int(i)], f\"{int(i):02d}\") for i in pred_ids]\n",
    "    \n",
    "    # ğŸ”¥ ìˆ˜ì •: conversation ì»¬ëŸ¼ ì¶”ê°€\n",
    "    sub = pd.DataFrame({\n",
    "        \"idx\": test_df[\"idx\"],\n",
    "        \n",
    "        \"class\": pred_codes\n",
    "    })\n",
    "    \n",
    "    out_path = os.path.join(CFG.out_dir, f\"submission_{CFG.model_type}.csv\")\n",
    "    sub.to_csv(out_path, index=False)\n",
    "    \n",
    "    print(f\"\\nâœ… Submission saved: {out_path}\")\n",
    "    print(f\"Shape: {sub.shape}\")\n",
    "    print(f\"\\nSample predictions:\")\n",
    "    print(sub.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jedi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
