{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "829a809b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\C\\anaconda3\\envs\\jedi\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bertcnn] Epoch 1/5  loss=0.7510  val_macro_f1=0.9122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bertcnn] Epoch 2/5  loss=0.2144  val_macro_f1=0.9206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bertcnn] Epoch 3/5  loss=0.1109  val_macro_f1=0.9302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bertcnn] Epoch 4/5  loss=0.0336  val_macro_f1=0.9288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bertcnn] Epoch 5/5  loss=0.0142  val_macro_f1=0.9340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predict: 100%|██████████████████████████████████████████████████████| 32/32 [00:01<00:00, 18.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: outputs\\submission_bertcnn.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run_cnn_baselines.py\n",
    "# PyTorch CNN baselines for text classification\n",
    "# - \"bertcnn\": BERT encoder + CNN head (recommended)\n",
    "# - \"textcnn\": lightweight baseline\n",
    "# Fix: auto-cap max_len to model's max_position_embeddings (e.g., KcBERT=300)\n",
    "\n",
    "import os, math, random, numpy as np, pandas as pd, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoConfig, set_seed\n",
    ")\n",
    "\n",
    "# ---------------- CFG ----------------\n",
    "class CFG:\n",
    "    model_type   = \"bertcnn\"          # \"bertcnn\" or \"textcnn\"\n",
    "    bert_name    = \"beomi/KcBERT-base\"\n",
    "    max_len      = 512                # will be capped to model_limit automatically\n",
    "    batch_size   = 16\n",
    "    epochs       = 5\n",
    "    lr           = 2e-5               # for bertcnn\n",
    "    lr_textcnn   = 1e-3               # for textcnn\n",
    "    weight_decay = 0.01\n",
    "    val_ratio    = 0.15\n",
    "    seed         = 2025\n",
    "    data_dir     = \"data\"\n",
    "    out_dir      = \"outputs\"\n",
    "    device       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "os.makedirs(CFG.out_dir, exist_ok=True)\n",
    "set_seed(CFG.seed)\n",
    "device = torch.device(CFG.device)\n",
    "\n",
    "# --------- IO ----------\n",
    "def load_train_df(path):\n",
    "    df = pd.read_csv(path)\n",
    "    # expects columns: conversation, class\n",
    "    df = df.rename(columns={\"conversation\":\"text\",\"class\":\"label\"})\n",
    "    df[\"text\"] = df[\"text\"].astype(str).str.replace(\"\\n\", \" \")\n",
    "    df[\"label\"] = df[\"label\"].astype(str)\n",
    "    return df\n",
    "\n",
    "def load_test_df(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"text\"] = df[\"conversation\"].astype(str).str.replace(\"\\n\", \" \")\n",
    "    return df\n",
    "\n",
    "# 제출 코드 매핑(필요 시 수정)\n",
    "NAME2CODE = {\n",
    "    '협박 대화': '00',\n",
    "    '갈취 대화': '01',\n",
    "    '직장 내 괴롭힘 대화': '02',\n",
    "    '기타 괴롭힘 대화': '03',\n",
    "    '일반 대화': '04',\n",
    "}\n",
    "\n",
    "# -------- Tokenizer & Collate --------\n",
    "def prepare_tokenizer_and_maxlen():\n",
    "    # cap CFG.max_len to model's max_position_embeddings\n",
    "    cfg = AutoConfig.from_pretrained(CFG.bert_name)\n",
    "    model_limit = getattr(cfg, \"max_position_embeddings\", 512)\n",
    "    CFG.max_len = min(CFG.max_len, model_limit)\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(CFG.bert_name, use_fast=True)\n",
    "    tok.model_max_length = CFG.max_len\n",
    "    return tok\n",
    "\n",
    "def make_collate(tokenizer):\n",
    "    def _fn(batch):\n",
    "        texts, labels = zip(*batch)\n",
    "        enc = tokenizer(\n",
    "            list(texts),\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=CFG.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        y = torch.tensor(labels, dtype=torch.long)\n",
    "        return {\"input_ids\": enc[\"input_ids\"], \"attention_mask\": enc[\"attention_mask\"]}, y\n",
    "    return _fn\n",
    "\n",
    "class RawDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, i):\n",
    "        y = -1 if self.labels is None else self.labels[i]\n",
    "        return self.texts[i], y\n",
    "\n",
    "# -------- Models --------\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, pad_id, num_labels, emb_dim=300, kernels=(3,4,5), channels=128, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_id)\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(emb_dim, channels, k) for k in kernels])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(channels * len(kernels), num_labels)\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.emb(input_ids)              # [B, L, E]\n",
    "        x = x.transpose(1, 2)                # [B, E, L]\n",
    "        feats = []\n",
    "        for conv in self.convs:\n",
    "            h = F.relu(conv(x))              # [B, C, L']\n",
    "            if attention_mask is not None:\n",
    "                Lp = h.size(-1)\n",
    "                mask = attention_mask[:, :Lp].unsqueeze(1)  # [B,1,L']\n",
    "                h = h.masked_fill(mask == 0, float(\"-inf\"))\n",
    "            h = torch.max(h, dim=-1).values  # Global Max Pool -> [B, C]\n",
    "            feats.append(h)\n",
    "        z = torch.cat(feats, dim=1)          # [B, C*len(k)]\n",
    "        z = self.dropout(z)\n",
    "        return self.fc(z)                    # [B, num_labels]\n",
    "\n",
    "class BertCnnHead(nn.Module):\n",
    "    def __init__(self, bert_name, num_labels, kernels=(2,3,4), channels=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(bert_name)\n",
    "        hid = self.backbone.config.hidden_size\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(hid, channels, k) for k in kernels])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(channels * len(kernels), num_labels)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        H = out.last_hidden_state              # [B, L, H]\n",
    "        x = H.transpose(1, 2)                  # [B, H, L]\n",
    "        feats = []\n",
    "        for conv in self.convs:\n",
    "            h = F.relu(conv(x))                # [B, C, L']\n",
    "            Lp = h.size(-1)\n",
    "            mask = attention_mask[:, :Lp].unsqueeze(1)  # [B,1,L']\n",
    "            h = h.masked_fill(mask == 0, float(\"-inf\"))\n",
    "            h = torch.max(h, dim=-1).values    # [B, C]\n",
    "            feats.append(h)\n",
    "        z = torch.cat(feats, dim=1)\n",
    "        z = self.dropout(z)\n",
    "        return self.fc(z)                      # [B, num_labels]\n",
    "\n",
    "# -------- Train/Eval --------\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    for enc, y in loader:\n",
    "        enc = {k:v.to(device) for k,v in enc.items()}\n",
    "        y = y.to(device)\n",
    "        logits = model(**enc)\n",
    "        ps.append(logits.softmax(1).cpu())\n",
    "        ys.append(y.cpu())\n",
    "    P = torch.cat(ps).numpy()\n",
    "    Y = torch.cat(ys).numpy()\n",
    "    preds = P.argmax(1)\n",
    "    f1 = f1_score(Y, preds, average=\"macro\")\n",
    "    return f1, Y, preds\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, scheduler=None):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for enc, y in tqdm(loader, ncols=100, leave=False):\n",
    "        enc = {k:v.to(device) for k,v in enc.items()}\n",
    "        y = y.to(device)\n",
    "        logits = model(**enc)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        if scheduler: scheduler.step()\n",
    "        total += loss.item()\n",
    "    return total / max(1, len(loader))\n",
    "\n",
    "# -------- Main --------\n",
    "def main():\n",
    "    # 1) Data\n",
    "    train_df = load_train_df(os.path.join(CFG.data_dir, \"train.csv\"))\n",
    "    test_df  = load_test_df(os.path.join(CFG.data_dir,  \"test.csv\"))\n",
    "\n",
    "    # labels\n",
    "    # keep the fixed NAME2CODE order if present\n",
    "    def order_key(name):\n",
    "        return list(NAME2CODE.keys()).index(name) if name in NAME2CODE else 999\n",
    "    labels = sorted(train_df[\"label\"].unique().tolist(), key=order_key)\n",
    "    lid = {l:i for i,l in enumerate(labels)}\n",
    "    id2code = {i: NAME2CODE.get(labels[i], f\"{i:02d}\") for i in range(len(labels))}\n",
    "    y_all = np.array([lid[l] for l in train_df[\"label\"].tolist()])\n",
    "    texts = train_df[\"text\"].tolist()\n",
    "\n",
    "    # 2) tokenizer (with max_len cap)\n",
    "    tok = prepare_tokenizer_and_maxlen()\n",
    "\n",
    "    # 3) stratified split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=CFG.val_ratio, random_state=CFG.seed)\n",
    "    tr_idx, va_idx = next(sss.split(np.arange(len(texts)), y_all))\n",
    "    tr_texts, va_texts = [texts[i] for i in tr_idx], [texts[i] for i in va_idx]\n",
    "    tr_labels, va_labels = y_all[tr_idx].tolist(), y_all[va_idx].tolist()\n",
    "\n",
    "    # 4) datasets/loaders\n",
    "    collate = make_collate(tok)\n",
    "    train_ds = RawDataset(tr_texts, tr_labels)\n",
    "    val_ds   = RawDataset(va_texts, va_labels)\n",
    "    test_ds  = RawDataset(test_df[\"text\"].tolist(), None)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True,  collate_fn=collate, num_workers=0)\n",
    "    val_loader   = torch.utils.data.DataLoader(val_ds,   batch_size=CFG.batch_size, shuffle=False, collate_fn=collate, num_workers=0)\n",
    "    test_loader  = torch.utils.data.DataLoader(test_ds,  batch_size=CFG.batch_size, shuffle=False, collate_fn=collate, num_workers=0)\n",
    "\n",
    "    # 5) model\n",
    "    if CFG.model_type == \"textcnn\":\n",
    "        model = TextCNN(\n",
    "            vocab_size=tok.vocab_size,\n",
    "            pad_id=tok.pad_token_id if tok.pad_token_id is not None else 0,\n",
    "            num_labels=len(labels),\n",
    "            emb_dim=300, kernels=(3,4,5), channels=128, dropout=0.2\n",
    "        )\n",
    "        lr = CFG.lr_textcnn\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "        scheduler = None\n",
    "    else:\n",
    "        model = BertCnnHead(CFG.bert_name, num_labels=len(labels), kernels=(2,3,4), channels=128, dropout=0.1)\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        grouped = [\n",
    "            {\"params\":[p for n,p in model.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\":CFG.weight_decay},\n",
    "            {\"params\":[p for n,p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\":0.0},\n",
    "        ]\n",
    "        lr = CFG.lr\n",
    "        optimizer = torch.optim.AdamW(grouped, lr=lr)\n",
    "        total_steps = math.ceil(len(train_loader) * CFG.epochs)\n",
    "        # simple linear warmup→decay\n",
    "        warmup = max(1, int(0.1 * total_steps))\n",
    "        scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "            optimizer,\n",
    "            schedulers=[\n",
    "                torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, total_iters=warmup),\n",
    "                torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=total_steps - warmup),\n",
    "            ],\n",
    "            milestones=[warmup]\n",
    "        )\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 6) train\n",
    "    best_f1, best_state = -1.0, None\n",
    "    for ep in range(1, CFG.epochs+1):\n",
    "        tr_loss = train_one_epoch(model, train_loader, optimizer, scheduler)\n",
    "        f1, y_true, y_pred = evaluate(model, val_loader)\n",
    "        print(f\"[{CFG.model_type}] Epoch {ep}/{CFG.epochs}  loss={tr_loss:.4f}  val_macro_f1={f1:.4f}\")\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_state = f1, {k:v.cpu() for k,v in model.state_dict().items()}\n",
    "            report = classification_report(y_true, y_pred, target_names=labels, digits=4)\n",
    "\n",
    "    # 7) save and predict\n",
    "    torch.save({\"state_dict\": best_state, \"labels\": labels}, os.path.join(CFG.out_dir, f\"{CFG.model_type}_best.pt\"))\n",
    "    with open(os.path.join(CFG.out_dir, f\"{CFG.model_type}_val_report.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "    # load best to eval\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for enc, _ in tqdm(test_loader, ncols=100, desc=\"Predict\"):\n",
    "            enc = {k:v.to(device) for k,v in enc.items()}\n",
    "            logits = model(**enc)\n",
    "            preds.append(logits.argmax(1).cpu())\n",
    "    pred_ids = torch.cat(preds).numpy()\n",
    "    pred_codes = [id2code[int(i)] for i in range(len(labels))]  # ensure mapping built\n",
    "    pred_codes = [NAME2CODE.get(labels[i], f\"{i:02d}\") for i in pred_ids]\n",
    "\n",
    "    sub = pd.DataFrame({\"idx\": test_df[\"idx\"], \"class\": pred_codes})\n",
    "    out_path = os.path.join(CFG.out_dir, f\"submission_{CFG.model_type}.csv\")\n",
    "    sub.to_csv(out_path, index=False)\n",
    "    print(\"saved:\", out_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jedi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
